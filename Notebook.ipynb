{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook du micro-projet\n",
    "## But du dataset:\n",
    "Ce fichier est disponible sur le git : [GitHub Project language_of_sign_recognition](https://github.com/morvan-s/language_of_sign_recognition)\n",
    "\n",
    "Ce fichier récupère et prépare les données pour qu'elles soit exploité par le réseau de neurone.\n",
    "\n",
    "## Déroulement du code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par bon nombre d'import. Principalement lié à Keras et Tensorflow (dont Keras est une couche).\n",
    "Jusque là  en se qui concerne le code, tout va bien.\n",
    "\n",
    "On commence alors par aller chercher les différentes images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load images\n",
    "    images = np.load('datasets/images.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces images sont dans images.npy. Ensuite on met la valeur indiquer par chacune des images. Comme ça on pourra vérifier sir le réseau a bon ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0d192697b54e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m204\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m204\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m409\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m409\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m615\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m615\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m822\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "    classes = np.zeros(images.shape[0])\n",
    "    classes[:204] = 9\n",
    "    classes[204:409] = 0\n",
    "    classes[409:615] = 7\n",
    "    classes[615:822] = 6\n",
    "    classes[822:1028] = 1\n",
    "    classes[1028:1236] = 8\n",
    "    classes[1236:1443] = 4\n",
    "    classes[1443:1649] = 3\n",
    "    classes[1649:1855] = 2\n",
    "    classes[1855:] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on recadre bien les images pour donné un \"norme\" pour facilité tous les calculs/comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    classes = np_utils.to_categorical(classes, 10)\n",
    "    images = images.reshape(images.shape[0], 64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On mélange bien les valeurs qui était jusque là trié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    combined = list(zip(images, classes))\n",
    "    random.shuffle(combined)\n",
    "    images[:], classes[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fini en séparant en 80% des données pour l'entraînement et 20% pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    splitIndex = int(len(images) * 0.8)\n",
    "    x_train = images[:splitIndex]\n",
    "    x_test = images[splitIndex:]\n",
    "    y_train = classes[:splitIndex]\n",
    "    y_test = classes[splitIndex:]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But du modèle :\n",
    "Ce fichier est disponible sur le git : [GitHub Project language_of_sign_recognition](https://github.com/morvan-s/language_of_sign_recognition)\n",
    "\n",
    "Ce fichier centralise une grande partie de se projet et en particulier le model qui va réunir les différentes couches du réseau.\n",
    "\n",
    "## Déroulement du code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Convolution2D(64, (2, 2), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Convolution2D(128, (2, 2), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Convolution2D(256, (2, 2), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire sur le model**\n",
    "\n",
    "Ensuite on compile ce model, pour ce faire on a 3 paramètres, en particulier l'optimizer (adam, optimiseur stochastique) et le loss (crossentropy car ce ne sont pas de simple entier) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a ensuite un load. Il appel une méthode de l'autre fichier python (DataSet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = dataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous avons 4 paramètres d'initialiser :\n",
    "- Epochs : Le nombre d'époque utiliser. Plus cette valeur est élever plus le réseau sera efficace. Mais plus le code sera long à  exécuter.\n",
    "- Batch_size : La taille des différents lots.\n",
    "- Data_augmentation : Comme son nom l'indique, ce booléen indique si on utilise la data augmentation (multiplié le nombre de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "DATA_AUGMENTATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, en fonction de la nécessité ou non d'augmenter le nombre données, on va ou non créer un générateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_AUGMENTATION:\n",
    "    # With data augmentation\n",
    "    generator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        zoom_range=[0.7, 1.3],\n",
    "    )\n",
    "    model.fit_generator(\n",
    "        generator.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "        steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=generator.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "    )\n",
    "else:\n",
    "    # Without data augmentation\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le générateur est un ImageDataGenerator, générateur intégrer à Keras. Ici on a pris quelques décision pour l'augmentation tel que la rotation des images et l'intervalle de zoom. On utilise pour finir le model créer précédemment pour entraîné le réseau.\n",
    "Pour finir on va tester l'efficacité du réseau de neurone grâce à la méthode evaluate du model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
