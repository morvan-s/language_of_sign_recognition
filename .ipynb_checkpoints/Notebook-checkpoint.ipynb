{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook du micro-projet\n",
    "## But du dataset:\n",
    "Le fichier est disponible sur le git avec le nom \"dataset.py\" : [GitHub Project language_of_sign_recognition](https://github.com/morvan-s/language_of_sign_recognition)\n",
    "\n",
    "Ce fichier récupère et prépare les données pour qu'elles soit exploité par le réseau de neurone.\n",
    "\n",
    "## Déroulement du code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par bon nombre d'import. Principalement lié à Keras et Tensorflow (dont Keras est une sur-couche).\n",
    "\n",
    "On va procéder a un formatage du jeu de données : on commence alors par aller chercher les différentes images dans images.npy. Ensuite on va assigner pour chaque image, le bon label (les labels fournis de base n'étant pas bien assigné). C'est ce qui va permettre d'entrainer le réseau et verifier qu'il a bien appris. Puis on va modifier le format des images avant de les passer dans le modele afin de faciliter tous les calculs/comparaison. Afin d'améliorer l'entrainement, on va mélanger le dataset, cela va permettre que le réseau ne traite pas toutes les valeurs de 9 puis toutes les valeurs de 0 etc. Cela améliorer les performances lors de l'entrainement, on fini en séparant en 80% des données pour l'entraînement et 20% pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load images\n",
    "    images = np.load('datasets/images.npy')\n",
    "    classes = np.zeros(images.shape[0])\n",
    "    classes[:204] = 9\n",
    "    classes[204:409] = 0\n",
    "    classes[409:615] = 7\n",
    "    classes[615:822] = 6\n",
    "    classes[822:1028] = 1\n",
    "    classes[1028:1236] = 8\n",
    "    classes[1236:1443] = 4\n",
    "    classes[1443:1649] = 3\n",
    "    classes[1649:1855] = 2\n",
    "    classes[1855:] = 5\n",
    "    \n",
    "    classes = np_utils.to_categorical(classes, 10)\n",
    "    images = images.reshape(images.shape[0], 64, 64, 1)\n",
    "    \n",
    "    combined = list(zip(images, classes))\n",
    "    random.shuffle(combined)\n",
    "    images[:], classes[:] = zip(*combined)\n",
    "    \n",
    "    splitIndex = int(len(images) * 0.8)\n",
    "    x_train = images[:splitIndex]\n",
    "    x_test = images[splitIndex:]\n",
    "    y_train = classes[:splitIndex]\n",
    "    y_test = classes[splitIndex:]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But du modèle :\n",
    "Ce fichier est disponible sur le git avec le nom model.py : [GitHub Project language_of_sign_recognition](https://github.com/morvan-s/language_of_sign_recognition)\n",
    "\n",
    "Ce fichier centralise une grande partie de se projet et en particulier le model qui va réunir les différentes couches du réseau.\n",
    "\n",
    "## Déroulement du code :\n",
    "\n",
    "Dans cette partie, on va comparer les résultats d'un modèle classique avec ceux d'un modèle convolutionnel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle classique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1649 samples, validate on 413 samples\n",
      "Epoch 1/100\n",
      "1649/1649 [==============================] - 1s 413us/step - loss: 2.3149 - acc: 0.1195 - val_loss: 2.2151 - val_acc: 0.1162\n",
      "Epoch 2/100\n",
      "1649/1649 [==============================] - 0s 118us/step - loss: 2.2012 - acc: 0.1704 - val_loss: 2.0920 - val_acc: 0.2349\n",
      "Epoch 3/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 2.1173 - acc: 0.2292 - val_loss: 2.0198 - val_acc: 0.2833\n",
      "Epoch 4/100\n",
      "1649/1649 [==============================] - 0s 116us/step - loss: 2.0716 - acc: 0.2147 - val_loss: 1.9105 - val_acc: 0.4044\n",
      "Epoch 5/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 1.9018 - acc: 0.3384 - val_loss: 1.7328 - val_acc: 0.4455\n",
      "Epoch 6/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 1.8012 - acc: 0.3766 - val_loss: 1.9637 - val_acc: 0.2809\n",
      "Epoch 7/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 1.7845 - acc: 0.3602 - val_loss: 1.6791 - val_acc: 0.4286\n",
      "Epoch 8/100\n",
      "1649/1649 [==============================] - 0s 98us/step - loss: 1.7266 - acc: 0.3881 - val_loss: 1.5370 - val_acc: 0.4697\n",
      "Epoch 9/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 1.6314 - acc: 0.4482 - val_loss: 1.4889 - val_acc: 0.4818\n",
      "Epoch 10/100\n",
      "1649/1649 [==============================] - 0s 105us/step - loss: 1.5591 - acc: 0.4809 - val_loss: 1.3743 - val_acc: 0.5763\n",
      "Epoch 11/100\n",
      "1649/1649 [==============================] - 0s 100us/step - loss: 1.5156 - acc: 0.4955 - val_loss: 1.3373 - val_acc: 0.5738\n",
      "Epoch 12/100\n",
      "1649/1649 [==============================] - 0s 99us/step - loss: 1.4684 - acc: 0.5021 - val_loss: 1.2885 - val_acc: 0.5860\n",
      "Epoch 13/100\n",
      "1649/1649 [==============================] - 0s 116us/step - loss: 1.4690 - acc: 0.4979 - val_loss: 1.2533 - val_acc: 0.6150\n",
      "Epoch 14/100\n",
      "1649/1649 [==============================] - 0s 111us/step - loss: 1.3876 - acc: 0.5324 - val_loss: 1.2506 - val_acc: 0.5884\n",
      "Epoch 15/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 1.3548 - acc: 0.5464 - val_loss: 1.2292 - val_acc: 0.5811\n",
      "Epoch 16/100\n",
      "1649/1649 [==============================] - 0s 111us/step - loss: 1.3218 - acc: 0.5500 - val_loss: 1.2307 - val_acc: 0.5981\n",
      "Epoch 17/100\n",
      "1649/1649 [==============================] - 0s 105us/step - loss: 1.2922 - acc: 0.5561 - val_loss: 1.1054 - val_acc: 0.6174\n",
      "Epoch 18/100\n",
      "1649/1649 [==============================] - 0s 118us/step - loss: 1.2198 - acc: 0.5907 - val_loss: 1.0986 - val_acc: 0.6320\n",
      "Epoch 19/100\n",
      "1649/1649 [==============================] - 0s 111us/step - loss: 1.1791 - acc: 0.6076 - val_loss: 1.0287 - val_acc: 0.6610\n",
      "Epoch 20/100\n",
      "1649/1649 [==============================] - 0s 108us/step - loss: 1.1395 - acc: 0.6216 - val_loss: 1.0343 - val_acc: 0.6634\n",
      "Epoch 21/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 1.1230 - acc: 0.6252 - val_loss: 1.0882 - val_acc: 0.6126\n",
      "Epoch 22/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 1.0779 - acc: 0.6434 - val_loss: 1.1054 - val_acc: 0.6174\n",
      "Epoch 23/100\n",
      "1649/1649 [==============================] - 0s 103us/step - loss: 1.0842 - acc: 0.6519 - val_loss: 0.9721 - val_acc: 0.6634\n",
      "Epoch 24/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 1.0949 - acc: 0.6277 - val_loss: 1.0670 - val_acc: 0.6465\n",
      "Epoch 25/100\n",
      "1649/1649 [==============================] - 0s 109us/step - loss: 1.0501 - acc: 0.6543 - val_loss: 0.9471 - val_acc: 0.7022\n",
      "Epoch 26/100\n",
      "1649/1649 [==============================] - 0s 118us/step - loss: 0.9812 - acc: 0.6877 - val_loss: 0.9075 - val_acc: 0.6973\n",
      "Epoch 27/100\n",
      "1649/1649 [==============================] - 0s 124us/step - loss: 1.0512 - acc: 0.6477 - val_loss: 1.1308 - val_acc: 0.6077\n",
      "Epoch 28/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 1.0636 - acc: 0.6325 - val_loss: 0.8441 - val_acc: 0.7215\n",
      "Epoch 29/100\n",
      "1649/1649 [==============================] - 0s 118us/step - loss: 0.9350 - acc: 0.7004 - val_loss: 0.8526 - val_acc: 0.7288\n",
      "Epoch 30/100\n",
      "1649/1649 [==============================] - 0s 106us/step - loss: 0.9615 - acc: 0.6780 - val_loss: 0.8185 - val_acc: 0.7288\n",
      "Epoch 31/100\n",
      "1649/1649 [==============================] - 0s 107us/step - loss: 0.9150 - acc: 0.6919 - val_loss: 0.8109 - val_acc: 0.7385\n",
      "Epoch 32/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 0.9454 - acc: 0.6683 - val_loss: 0.7956 - val_acc: 0.7506\n",
      "Epoch 33/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 0.8961 - acc: 0.6992 - val_loss: 0.8429 - val_acc: 0.7312\n",
      "Epoch 34/100\n",
      "1649/1649 [==============================] - 0s 118us/step - loss: 0.8772 - acc: 0.7113 - val_loss: 0.7770 - val_acc: 0.7458\n",
      "Epoch 35/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 0.8438 - acc: 0.7229 - val_loss: 0.7865 - val_acc: 0.7264\n",
      "Epoch 36/100\n",
      "1649/1649 [==============================] - 0s 111us/step - loss: 0.8889 - acc: 0.6913 - val_loss: 0.7894 - val_acc: 0.7627\n",
      "Epoch 37/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 0.8546 - acc: 0.7162 - val_loss: 0.7916 - val_acc: 0.7385\n",
      "Epoch 38/100\n",
      "1649/1649 [==============================] - 0s 108us/step - loss: 0.8541 - acc: 0.7198 - val_loss: 0.7578 - val_acc: 0.7482\n",
      "Epoch 39/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 0.8339 - acc: 0.7283 - val_loss: 0.7485 - val_acc: 0.7530\n",
      "Epoch 40/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 0.8322 - acc: 0.7065 - val_loss: 0.7330 - val_acc: 0.7579\n",
      "Epoch 41/100\n",
      "1649/1649 [==============================] - 0s 107us/step - loss: 0.7904 - acc: 0.7362 - val_loss: 0.7191 - val_acc: 0.7482\n",
      "Epoch 42/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 0.8143 - acc: 0.7186 - val_loss: 0.7325 - val_acc: 0.7603\n",
      "Epoch 43/100\n",
      "1649/1649 [==============================] - 0s 104us/step - loss: 0.8058 - acc: 0.7223 - val_loss: 0.7433 - val_acc: 0.7724\n",
      "Epoch 44/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 0.8327 - acc: 0.7229 - val_loss: 0.9784 - val_acc: 0.6828\n",
      "Epoch 45/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 0.7939 - acc: 0.7362 - val_loss: 0.8935 - val_acc: 0.6973\n",
      "Epoch 46/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 0.7946 - acc: 0.7295 - val_loss: 0.7877 - val_acc: 0.7530\n",
      "Epoch 47/100\n",
      "1649/1649 [==============================] - 0s 114us/step - loss: 0.7627 - acc: 0.7374 - val_loss: 0.7999 - val_acc: 0.7288\n",
      "Epoch 48/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 0.7607 - acc: 0.7423 - val_loss: 0.7457 - val_acc: 0.7627\n",
      "Epoch 49/100\n",
      "1649/1649 [==============================] - 0s 117us/step - loss: 0.7300 - acc: 0.7477 - val_loss: 0.6845 - val_acc: 0.7748\n",
      "Epoch 50/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 0.7307 - acc: 0.7562 - val_loss: 0.7635 - val_acc: 0.7458\n",
      "Epoch 51/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 0.7068 - acc: 0.7617 - val_loss: 0.7017 - val_acc: 0.7700\n",
      "Epoch 52/100\n",
      "1649/1649 [==============================] - 0s 104us/step - loss: 0.7751 - acc: 0.7423 - val_loss: 0.6648 - val_acc: 0.7772\n",
      "Epoch 53/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 0.7961 - acc: 0.7247 - val_loss: 0.6864 - val_acc: 0.7676\n",
      "Epoch 54/100\n",
      "1649/1649 [==============================] - 0s 108us/step - loss: 0.6820 - acc: 0.7732 - val_loss: 0.6942 - val_acc: 0.7772\n",
      "Epoch 55/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 0.6718 - acc: 0.7750 - val_loss: 0.6768 - val_acc: 0.7845\n",
      "Epoch 56/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 0.7184 - acc: 0.7477 - val_loss: 0.8342 - val_acc: 0.7070\n",
      "Epoch 57/100\n",
      "1649/1649 [==============================] - 0s 103us/step - loss: 0.7028 - acc: 0.7586 - val_loss: 0.6500 - val_acc: 0.7869\n",
      "Epoch 58/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 0.6634 - acc: 0.7665 - val_loss: 0.6554 - val_acc: 0.7918\n",
      "Epoch 59/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 0.6777 - acc: 0.7702 - val_loss: 0.6469 - val_acc: 0.7797\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1649/1649 [==============================] - 0s 114us/step - loss: 0.6593 - acc: 0.7841 - val_loss: 1.0115 - val_acc: 0.6441\n",
      "Epoch 61/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 0.7367 - acc: 0.7477 - val_loss: 0.6784 - val_acc: 0.7700\n",
      "Epoch 62/100\n",
      "1649/1649 [==============================] - 0s 127us/step - loss: 0.6708 - acc: 0.7659 - val_loss: 0.7115 - val_acc: 0.7700\n",
      "Epoch 63/100\n",
      "1649/1649 [==============================] - 0s 112us/step - loss: 0.6895 - acc: 0.7544 - val_loss: 0.6586 - val_acc: 0.7651\n",
      "Epoch 64/100\n",
      "1649/1649 [==============================] - 0s 114us/step - loss: 0.6775 - acc: 0.7653 - val_loss: 0.8902 - val_acc: 0.6901\n",
      "Epoch 65/100\n",
      "1649/1649 [==============================] - 0s 109us/step - loss: 0.6546 - acc: 0.7659 - val_loss: 0.8232 - val_acc: 0.7191\n",
      "Epoch 66/100\n",
      "1649/1649 [==============================] - 0s 105us/step - loss: 0.6326 - acc: 0.7853 - val_loss: 0.5921 - val_acc: 0.8039\n",
      "Epoch 67/100\n",
      "1649/1649 [==============================] - 0s 114us/step - loss: 0.6183 - acc: 0.7768 - val_loss: 0.7033 - val_acc: 0.7676\n",
      "Epoch 68/100\n",
      "1649/1649 [==============================] - 0s 108us/step - loss: 0.6184 - acc: 0.7865 - val_loss: 0.6281 - val_acc: 0.7724\n",
      "Epoch 69/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 0.6293 - acc: 0.7847 - val_loss: 0.6045 - val_acc: 0.8063\n",
      "Epoch 70/100\n",
      "1649/1649 [==============================] - 0s 109us/step - loss: 0.6041 - acc: 0.7871 - val_loss: 0.5866 - val_acc: 0.8039\n",
      "Epoch 71/100\n",
      "1649/1649 [==============================] - 0s 104us/step - loss: 0.6430 - acc: 0.7829 - val_loss: 0.7824 - val_acc: 0.7215\n",
      "Epoch 72/100\n",
      "1649/1649 [==============================] - 0s 113us/step - loss: 0.6516 - acc: 0.7696 - val_loss: 0.5857 - val_acc: 0.7990\n",
      "Epoch 73/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 0.6113 - acc: 0.7841 - val_loss: 0.5853 - val_acc: 0.8184\n",
      "Epoch 74/100\n",
      "1649/1649 [==============================] - 0s 103us/step - loss: 0.6271 - acc: 0.7823 - val_loss: 0.6220 - val_acc: 0.7893\n",
      "Epoch 75/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 0.6566 - acc: 0.7702 - val_loss: 0.6046 - val_acc: 0.7990\n",
      "Epoch 76/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 0.5737 - acc: 0.8035 - val_loss: 0.9163 - val_acc: 0.6804\n",
      "Epoch 77/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 0.5998 - acc: 0.7865 - val_loss: 0.5866 - val_acc: 0.8087\n",
      "Epoch 78/100\n",
      "1649/1649 [==============================] - 0s 114us/step - loss: 0.5803 - acc: 0.8053 - val_loss: 0.6656 - val_acc: 0.7627\n",
      "Epoch 79/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 0.5736 - acc: 0.8047 - val_loss: 0.6554 - val_acc: 0.7772\n",
      "Epoch 80/100\n",
      "1649/1649 [==============================] - 0s 99us/step - loss: 0.5522 - acc: 0.8120 - val_loss: 0.6032 - val_acc: 0.8111\n",
      "Epoch 81/100\n",
      "1649/1649 [==============================] - 0s 107us/step - loss: 0.5926 - acc: 0.7920 - val_loss: 0.5560 - val_acc: 0.8160\n",
      "Epoch 82/100\n",
      "1649/1649 [==============================] - 0s 105us/step - loss: 0.5625 - acc: 0.8084 - val_loss: 0.6854 - val_acc: 0.7700\n",
      "Epoch 83/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 0.5474 - acc: 0.8235 - val_loss: 0.5518 - val_acc: 0.8136\n",
      "Epoch 84/100\n",
      "1649/1649 [==============================] - 0s 117us/step - loss: 0.5171 - acc: 0.8193 - val_loss: 0.7046 - val_acc: 0.7554\n",
      "Epoch 85/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 0.5570 - acc: 0.8017 - val_loss: 0.7575 - val_acc: 0.7119\n",
      "Epoch 86/100\n",
      "1649/1649 [==============================] - 0s 104us/step - loss: 0.5808 - acc: 0.7968 - val_loss: 0.5854 - val_acc: 0.7990\n",
      "Epoch 87/100\n",
      "1649/1649 [==============================] - 0s 104us/step - loss: 0.5313 - acc: 0.8193 - val_loss: 0.6083 - val_acc: 0.7821\n",
      "Epoch 88/100\n",
      "1649/1649 [==============================] - 0s 121us/step - loss: 0.5372 - acc: 0.8120 - val_loss: 0.5306 - val_acc: 0.8232\n",
      "Epoch 89/100\n",
      "1649/1649 [==============================] - 0s 115us/step - loss: 0.5437 - acc: 0.8120 - val_loss: 0.5692 - val_acc: 0.8160\n",
      "Epoch 90/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 0.5475 - acc: 0.8108 - val_loss: 0.6487 - val_acc: 0.7651\n",
      "Epoch 91/100\n",
      "1649/1649 [==============================] - 0s 106us/step - loss: 0.5335 - acc: 0.8132 - val_loss: 0.5788 - val_acc: 0.8015\n",
      "Epoch 92/100\n",
      "1649/1649 [==============================] - 0s 107us/step - loss: 0.5581 - acc: 0.7968 - val_loss: 0.5160 - val_acc: 0.8305\n",
      "Epoch 93/100\n",
      "1649/1649 [==============================] - 0s 106us/step - loss: 0.4841 - acc: 0.8308 - val_loss: 0.5491 - val_acc: 0.8232\n",
      "Epoch 94/100\n",
      "1649/1649 [==============================] - 0s 102us/step - loss: 0.4763 - acc: 0.8514 - val_loss: 0.5778 - val_acc: 0.8063\n",
      "Epoch 95/100\n",
      "1649/1649 [==============================] - 0s 111us/step - loss: 0.5601 - acc: 0.7890 - val_loss: 0.6581 - val_acc: 0.7700\n",
      "Epoch 96/100\n",
      "1649/1649 [==============================] - 0s 105us/step - loss: 0.5164 - acc: 0.8266 - val_loss: 0.7404 - val_acc: 0.7530\n",
      "Epoch 97/100\n",
      "1649/1649 [==============================] - 0s 109us/step - loss: 0.5009 - acc: 0.8308 - val_loss: 0.5121 - val_acc: 0.8305\n",
      "Epoch 98/100\n",
      "1649/1649 [==============================] - 0s 110us/step - loss: 0.4503 - acc: 0.8514 - val_loss: 0.5960 - val_acc: 0.8063\n",
      "Epoch 99/100\n",
      "1649/1649 [==============================] - 0s 101us/step - loss: 0.4511 - acc: 0.8587 - val_loss: 0.4932 - val_acc: 0.8208\n",
      "Epoch 100/100\n",
      "1649/1649 [==============================] - 0s 106us/step - loss: 0.5072 - acc: 0.8205 - val_loss: 0.5383 - val_acc: 0.8063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a7943c940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Flatten(input_shape=(64,64,1)))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        32,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle convolutionnel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Convolution2D(64, (2, 2), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Convolution2D(128, (2, 2), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Convolution2D(256, (2, 2), activation='relu', input_shape=(64, 64, 1), data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire sur le model**\n",
    "On applique un modele convolutionnel classique: on alterne des couches de convolutions avec un filtre (2,2), une fonction d'activation Relu, et le data_format 'channels_last' qui est  à mettre en corrélation avec le formatage des données effectué à l'étape précéde,t. On alterne donc ces couches convolutionnelles avec de couches de max-pooling avec toujours un filtre (2,2) : cela permet de faire du sous-échantillonage et ainsi permettre au réseau d'extraire des features à différents niveau d'abstractions. On rajoute une couche flatten permettant de faire la liaisons entre les couches convolutionneles et les couches denses, les couches denses permettent d'obtenir le résultat final, la fonction softmax permet d'interpreter le résultat comme une probabilité. On a également ajouter deux couches de Dropout qui permettent de désactiver de manière aléaoire des sorties de neurones, cela nous as permis d'améliorer nos résultats.\n",
    "\n",
    "Ensuite on compile ce model, pour ce faire on a 3 paramètres, en particulier l'optimizer (adam, optimiseur stochastique) et le loss (crossentropy car ce ne sont pas de simple entier) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a ensuite un load. Il appel une méthode de l'autre fichier python (DataSet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous avons 4 paramètres d'initialiser :\n",
    "- Epochs : Le nombre d'époque utiliser. Plus cette valeur est élever plus le réseau sera efficace. Mais plus le code sera long à  exécuter.\n",
    "- Batch_size : La taille des différents lots.\n",
    "- Data_augmentation : Comme son nom l'indique, ce booléen indique si on utilise la data augmentation (multiplié le nombre de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "DATA_AUGMENTATION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on va permettre l'augmentation de données, ce qui nous as permis de comparer les résultats avec/sans cette augmentation. L'augmentation de données fonctionne de la manière suivante : on va effectuer sur les images du dataset des transformations diverses (rotation, scaling...), cela permet d'avoir beaucoup plus de données et d'avoir un réseau de neurones qui apprendra a associer le bon label a une image par exemple un peu de travers ou autre. \n",
    "Nous avons réalisé une petite démonstration web à l'aide de TensorFlowJS, nous avons constaté de bien meilleur résultat avec l'augmentation de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1649 samples, validate on 413 samples\n",
      "Epoch 1/30\n",
      "1649/1649 [==============================] - 12s 7ms/step - loss: 2.2011 - acc: 0.1389 - val_loss: 2.1080 - val_acc: 0.2373\n",
      "Epoch 2/30\n",
      "1649/1649 [==============================] - 11s 7ms/step - loss: 2.0430 - acc: 0.2583 - val_loss: 1.4449 - val_acc: 0.5109\n",
      "Epoch 3/30\n",
      "1649/1649 [==============================] - 11s 6ms/step - loss: 1.1844 - acc: 0.5785 - val_loss: 0.7600 - val_acc: 0.7579\n",
      "Epoch 4/30\n",
      "1649/1649 [==============================] - 10s 6ms/step - loss: 0.6834 - acc: 0.7599 - val_loss: 0.5378 - val_acc: 0.8475\n",
      "Epoch 5/30\n",
      "1649/1649 [==============================] - 10s 6ms/step - loss: 0.4907 - acc: 0.8351 - val_loss: 0.4034 - val_acc: 0.8886\n",
      "Epoch 6/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.3821 - acc: 0.8678 - val_loss: 0.3658 - val_acc: 0.8886\n",
      "Epoch 7/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.3326 - acc: 0.8848 - val_loss: 0.2777 - val_acc: 0.9274\n",
      "Epoch 8/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.2651 - acc: 0.9115 - val_loss: 0.2158 - val_acc: 0.9564\n",
      "Epoch 9/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.2176 - acc: 0.9266 - val_loss: 0.1973 - val_acc: 0.9516\n",
      "Epoch 10/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.1824 - acc: 0.9430 - val_loss: 0.1586 - val_acc: 0.9661\n",
      "Epoch 11/30\n",
      "1649/1649 [==============================] - 10s 6ms/step - loss: 0.1474 - acc: 0.9576 - val_loss: 0.1096 - val_acc: 0.9782\n",
      "Epoch 12/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.1166 - acc: 0.9594 - val_loss: 0.1022 - val_acc: 0.9709\n",
      "Epoch 13/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0939 - acc: 0.9703 - val_loss: 0.1082 - val_acc: 0.9709\n",
      "Epoch 14/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.1200 - acc: 0.9576 - val_loss: 0.1124 - val_acc: 0.9588\n",
      "Epoch 15/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0980 - acc: 0.9697 - val_loss: 0.0702 - val_acc: 0.9806\n",
      "Epoch 16/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0802 - acc: 0.9745 - val_loss: 0.0522 - val_acc: 0.9903\n",
      "Epoch 17/30\n",
      "1649/1649 [==============================] - 10s 6ms/step - loss: 0.0742 - acc: 0.9757 - val_loss: 0.0808 - val_acc: 0.9709\n",
      "Epoch 18/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0942 - acc: 0.9679 - val_loss: 0.0398 - val_acc: 0.9903\n",
      "Epoch 19/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0532 - acc: 0.9836 - val_loss: 0.0424 - val_acc: 0.9782\n",
      "Epoch 20/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0640 - acc: 0.9794 - val_loss: 0.0602 - val_acc: 0.9758\n",
      "Epoch 21/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0628 - acc: 0.9812 - val_loss: 0.0418 - val_acc: 0.9879\n",
      "Epoch 22/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0464 - acc: 0.9848 - val_loss: 0.0367 - val_acc: 0.9855\n",
      "Epoch 23/30\n",
      "1649/1649 [==============================] - 10s 6ms/step - loss: 0.0447 - acc: 0.9897 - val_loss: 0.0319 - val_acc: 0.9903\n",
      "Epoch 24/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0484 - acc: 0.9848 - val_loss: 0.0360 - val_acc: 0.9927\n",
      "Epoch 25/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0499 - acc: 0.9848 - val_loss: 0.0157 - val_acc: 0.9927\n",
      "Epoch 26/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0357 - acc: 0.9885 - val_loss: 0.0130 - val_acc: 0.9879\n",
      "Epoch 27/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.0293 - val_acc: 0.9903\n",
      "Epoch 28/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0378 - acc: 0.9879 - val_loss: 0.0198 - val_acc: 0.9952\n",
      "Epoch 29/30\n",
      "1649/1649 [==============================] - 9s 6ms/step - loss: 0.0476 - acc: 0.9818 - val_loss: 0.0142 - val_acc: 0.9927\n",
      "Epoch 30/30\n",
      "1649/1649 [==============================] - 10s 6ms/step - loss: 0.0296 - acc: 0.9915 - val_loss: 0.0463 - val_acc: 0.9903\n"
     ]
    }
   ],
   "source": [
    "if DATA_AUGMENTATION:\n",
    "    # With data augmentation\n",
    "    generator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        zoom_range=[0.7, 1.3],\n",
    "    )\n",
    "    model.fit_generator(\n",
    "        generator.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "        steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "else:\n",
    "    # Without data augmentation\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le générateur est un ImageDataGenerator, générateur intégrer à Keras. Ici on a pris quelques décision pour l'augmentation tel que la rotation des images et l'intervalle de zoom. On utilise pour finir le model créer précédemment pour entraîné le réseau.\n",
    "Pour finir on va tester l'efficacité du réseau de neurone grâce à la méthode evaluate du model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 1s 2ms/step\n",
      "Test loss: 0.04630225122011025\n",
      "Test accuracy: 0.9903147699757869\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Démonstration :\n",
    "\n",
    "Une démonstration en ligne a été faite pour ce projet (il suffit d'afficher le fichier index.html du dossier demo dans le navigateur).\n",
    "Le gif ci-dessous montre le fonctionnement de cette démonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![demonstration](demonstration.gif \"demonstration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
